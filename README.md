![giphy](https://github.com/Malav5372/SuperMarioPPO/assets/144440737/f0db61b8-0e8e-410f-8ce3-5e59ca0e66fb)

### Introduction

Implemented with Proximal Policy Optimization (PPO), my Python source code showcases the application of AI in mastering Super Mario Bros. The focus lies on training an agent through the PPO algorithm as detailed in the Proximal Policy Optimization Algorithms paper. The agent's performance is notable, successfully completing 31 out of 32 levels, exceeding initial expectations. This achievement highlights the efficacy of AI in gaming scenarios.

## PPO in SuperMarioBros in action : 

<p align="left">
  <img src="Example/video-1-1.gif" width="250">
  <img src="Example/video-1-2.gif" width="250">
  <img src="Example/video-1-3.gif" width="250">
  <img src="Example/video-1-4.gif" width="250"><br/>
  <img src="Example/video-2-1.gif" width="250">
  <img src="Example/video-2-2.gif" width="250">
  <img src="Example/video-2-3.gif" width="250">
  <img src="Example/video-2-4.gif" width="250"><br/>
</p>


### Refer the paper
Proximal Policy Optimization (PPO) algorithm introduced in the paper **Proximal Policy Optimization Algorithms** [paper](https://arxiv.org/abs/1707.06347).
