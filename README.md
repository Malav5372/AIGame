![giphy](https://github.com/Malav5372/SuperMarioPPO/assets/144440737/f0db61b8-0e8e-410f-8ce3-5e59ca0e66fb)

### Introduction

Implemented with Proximal Policy Optimization (PPO), my Python source code showcases the application of AI in mastering Super Mario Bros. The focus lies on training an agent through the PPO algorithm as detailed in the [Proximal Policy Optimization Algorithms paper](https://r.search.yahoo.com/_ylt=AwrKAUdxxbxl2VEmLBu7HAx.;_ylu=Y29sbwNzZzMEcG9zAzEEdnRpZAMEc2VjA3Ny/RV=2/RE=1706898930/RO=10/RU=https%3a%2f%2farxiv.org%2fabs%2f1707.06347/RK=2/RS=wEdBCZWduN00VRaDukPtwOfWFoM-).

### Concept Overview of the algorithm :

Proximal Policy Optimization (PPO) is a reinforcement learning algorithm designed for training agents in environments where an intelligent agent must learn to make sequential decisions. Developed by OpenAI, PPO is specifically designed to strike a balance between sample efficiency, stability, and ease of implementation.

The primary goal of PPO is to optimize policies for decision-making in reinforcement learning tasks. A policy in this context refers to the strategy or set of rules that an agent uses to determine its actions in a given state of the environment. PPO focuses on iteratively updating the policy in a way that ensures the changes are not too drastic, preventing policy updates from leading to undesirable and potentially unstable outcomes.

### Some Examples : 

<p align="left">

#### world-1: 
  <img src="Example/w-1,s-1.gif" width="250">
  <img src="Example/w-1,s-2.gif" width="250">
  <img src="Example/w-1,s-3.gif" width="250">
  <img src="Example/w-1,s-4.gif" width="250"><br/>

#### world-2:
  <img src="Example/w-2,s-1.gif" width="250">
  <img src="Example/w-2,s-2.gif" width="250">
  <img src="Example/w-2,s-3.gif" width="250">
  <img src="Example/w-2,s-4.gif" width="250"><br/>
</p>


